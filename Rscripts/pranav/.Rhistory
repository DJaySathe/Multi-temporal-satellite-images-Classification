plotTif(pred_img_labels)
########
# Maximum Likelihood Classifier
# Student Name: Pranav Nawathe
# Student Unity ID: ppnawath
######
# Do not clear your workspace
# Do not set working directory in the code
require(mixtools) #for bivariate density plots/ellipses
# Run startup script to load functions to read geotiff files
source('./startup.R')
start.time <- Sys.time()
# Read data for training and testing
data <- readCSV()
# tr contains training data, first element of data
# te contains testing data, second element of data
tr <- data[[1]]
te <- data[[2]]
# Read the image(Geotiff file)
img <- readTif()
# Count the number of training instances
ntraining <- nrow(tr)
# Count the number of features
nfeatures <- ncol(tr) - 1
# Count the number of classes
nclasses <- length(unique(tr$Class))
# tr_splits is a list, where each element of the list contains
# information pertaining a single class
tr_splits <- split(tr, tr$Class)
# HINT: For the next three methods, see how split() and lapply() work
# This will give you insight into writing code for the following functions
# Function to compute the mean of each feature in a data frame
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# (nrows x (nfeatures + 1))
# Returns:
# A data frame containing means of each feature of size (1 x nfeatures)
# Don't forget to ignore class label column when computing mean
###
compute_means <- function(df){
mean = colMeans(df[-4])
}
# Function to compute the covariance matrix between features in a data frame
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# (nrows x (nfeatures + 1))
# Returns:
# Covariance matrix of size nfeatures * nfeatures
# Don't forget to ignore class label column when computing covariance matrix
###
compute_covs <- function(df){
covariance = cov(df[-4])
}
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# ntraining, the number of training data points
# Returns:
# Probability of class contained in df
###
compute_apriori <- function(df, ntraining){
apriori = nrow(df)/ntraining
}
# tr_means is a list containing mean vector of attributes per class.
tr_means <- lapply(tr_splits, compute_means)
# tr_covs is a list containing covariance matrices of attributes per class.
tr_covs <- lapply(tr_splits, compute_covs)
# tr_apriori is a list containing prior probabilities per class.
tr_apriori <- lapply(tr_splits, compute_apriori, ntraining)
# function to implement bivariate density plots (ellipses)
# Plot the training data points using only feat1 and feat2
# Using the means and covs you calculated in the previous steps,
# plot ellipses for feat1 and feat2.
# You are expected to generate 5 ellipses in the same plot, one for each class
# Distinguish ellipses corresponding to classes using the colors variable
# You should also generate a legend, indicating colors assigned to each class.
# Label x and y axes and provide a descriptive plot title.
# Here's an example for ellipse() usage:
# ellipse(mu = [mean vector of size 1 x 2],
#   sigma = [covariance matrix of size 2 x 2],
#   col = colors[class])
# An example plot has been provided in the PDF.
###
bvd <- function(feat1, feat2, tr_means = tr_means, tr_covs = tr_covs, colors = colors){
print(colors)
plot(x=tr[,feat1],y=tr[,feat2],xlab=rownames(data.frame(tr_means))[feat1],ylab=rownames(data.frame(tr_means))[feat2],main=paste("Plot for dimensions:",feat1,",",feat2))
legend(x="topleft",y=NULL,seq(1,nclasses),lty=c(1,1),col=colors,inset=c(0.005,0.005))
lapply(1:nclasses, function(i) lines(ellipse(mu = tr_means[[i]][c(feat1,feat2)],sigma = tr_covs[[i]][c(feat1,feat2),c(feat1,feat2)]),col=colors[i],lwd=i))
}
# Call the bvd density plot.
# 'feat1','feat2' variables refers to the dimensions
# Input dimension numbers. For eg: bvd(1, 2) refers to bvd
# over the first two features R and G.
bvd(1, 3, tr_means, tr_covs, colors)
# Implement MLC
# Note: The method must work for any number of dimensions
# Your code might be tested with an image with more than 3 features.
# Args:
# means - list containing mean vectors for each class
# covs - list containing covariance matrices for each class
# aprioris - list comprising of apriori probabilities for each class
# tedata - data frame of test data without the labels column
# Returns:
# predicted_cl - Vector of class assignments of length nrow(tedata)
# You should implement MLC manually, do not use any library
###
MLC <- function(means, covs, aprioris, tedata){
# vector to store predicted classes for each instance in test
predicted_cl = rep(0, nrow(tedata))
predicted_cl = apply(tedata, 1, MLC_calc, means=means,covs=covs,aprioris=aprioris)
#for (i in sequence(nrow(tedata))){
#}
# Return the vector of predicted labels
return(predicted_cl)
}
MLC_calc <- function(dataPoint, means, covs, aprioris){
#dataPoint = tedata[i,]
#print(dataPoint)
print(dim(transpose(as.matrix(dataPoint-means[[2]]))))
classProbs = rep(0, length(means))
classProbs = lapply(1:nclasses, function(j) (exp(-(as.matrix(dataPoint-means[[j]]))%*%solve(covs[[j]])%*%(t(dataPoint-means[[j]]))))*(aprioris[[j]]/sqrt(det(covs[[j]]))))
#for (j in sequence(length(means))){
#  classProbs[j] = (exp(-(as.matrix(dataPoint-means[[j]]))%*%solve(covs[[j]])%*%(t(dataPoint-means[[j]]))))*(aprioris[[j]]/sqrt(det(covs[[j]])))
#}
class = which.max(classProbs)
}
pred_te_labels <- MLC(tr_means, tr_covs, tr_apriori, te[, -ncol(te)])
# Generate confusion matrix - Variable 'tab' stores this confusion matrix
tab <- table(as.factor(te$Class), as.factor(pred_te_labels))
# Using the confusion matrix tab, compute overall accuracy
###
compute_overall_accuracy <- function(tab){
correct_classification<-0
total = 0
for(i in sequence(nrow(tab))){
correct_classification = correct_classification+tab[i,i]
total = total+sum(tab[i,])
}
return(correct_classification/total)
}
# Compute accuracy of a particular class using 'tab'
# Args:
# class - refers to the class number
# tab - confusion matrix
# Keep track of whether your actual and predicted labels are in rows or columns
# of tab
###
compute_class_accuracy <- function(class, tab){
return(tab[class]/sum(tab[class,]))
}
# Call overall class accuracy computation function
overall_te_accuracy <- compute_overall_accuracy(tab)
# Compute individual class accuracy for all classes
individual_te_accuracy <- sapply(1:nclasses, compute_class_accuracy, tab)
# Now, let's do it on the whole image
# This will take a few minutes (or longer, depending upon your implementation)
pred_img_labels <- MLC(tr_means, tr_covs, tr_apriori, img)
end.time <- Sys.time()
time.taken <- end.time - start.time
# Plot the classification result on entire image
# Check your working directory for 'mlc.tif'!
plotTif(pred_img_labels)
########
# Maximum Likelihood Classifier
# Student Name: Pranav Nawathe
# Student Unity ID: ppnawath
######
# Do not clear your workspace
# Do not set working directory in the code
require(mixtools) #for bivariate density plots/ellipses
# Run startup script to load functions to read geotiff files
source('./startup.R')
start.time <- Sys.time()
# Read data for training and testing
data <- readCSV()
# tr contains training data, first element of data
# te contains testing data, second element of data
tr <- data[[1]]
te <- data[[2]]
# Read the image(Geotiff file)
img <- readTif()
# Count the number of training instances
ntraining <- nrow(tr)
# Count the number of features
nfeatures <- ncol(tr) - 1
# Count the number of classes
nclasses <- length(unique(tr$Class))
# tr_splits is a list, where each element of the list contains
# information pertaining a single class
tr_splits <- split(tr, tr$Class)
# HINT: For the next three methods, see how split() and lapply() work
# This will give you insight into writing code for the following functions
# Function to compute the mean of each feature in a data frame
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# (nrows x (nfeatures + 1))
# Returns:
# A data frame containing means of each feature of size (1 x nfeatures)
# Don't forget to ignore class label column when computing mean
###
compute_means <- function(df){
mean = colMeans(df[-4])
}
# Function to compute the covariance matrix between features in a data frame
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# (nrows x (nfeatures + 1))
# Returns:
# Covariance matrix of size nfeatures * nfeatures
# Don't forget to ignore class label column when computing covariance matrix
###
compute_covs <- function(df){
covariance = cov(df[-4])
}
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# ntraining, the number of training data points
# Returns:
# Probability of class contained in df
###
compute_apriori <- function(df, ntraining){
apriori = nrow(df)/ntraining
}
# tr_means is a list containing mean vector of attributes per class.
tr_means <- lapply(tr_splits, compute_means)
# tr_covs is a list containing covariance matrices of attributes per class.
tr_covs <- lapply(tr_splits, compute_covs)
# tr_apriori is a list containing prior probabilities per class.
tr_apriori <- lapply(tr_splits, compute_apriori, ntraining)
# function to implement bivariate density plots (ellipses)
# Plot the training data points using only feat1 and feat2
# Using the means and covs you calculated in the previous steps,
# plot ellipses for feat1 and feat2.
# You are expected to generate 5 ellipses in the same plot, one for each class
# Distinguish ellipses corresponding to classes using the colors variable
# You should also generate a legend, indicating colors assigned to each class.
# Label x and y axes and provide a descriptive plot title.
# Here's an example for ellipse() usage:
# ellipse(mu = [mean vector of size 1 x 2],
#   sigma = [covariance matrix of size 2 x 2],
#   col = colors[class])
# An example plot has been provided in the PDF.
###
bvd <- function(feat1, feat2, tr_means = tr_means, tr_covs = tr_covs, colors = colors){
print(colors)
plot(x=tr[,feat1],y=tr[,feat2],xlab=rownames(data.frame(tr_means))[feat1],ylab=rownames(data.frame(tr_means))[feat2],main=paste("Plot for dimensions:",feat1,",",feat2))
legend(x="topleft",y=NULL,seq(1,nclasses),lty=c(1,1),col=colors,inset=c(0.005,0.005))
lapply(1:nclasses, function(i) lines(ellipse(mu = tr_means[[i]][c(feat1,feat2)],sigma = tr_covs[[i]][c(feat1,feat2),c(feat1,feat2)]),col=colors[i],lwd=i))
}
# Call the bvd density plot.
# 'feat1','feat2' variables refers to the dimensions
# Input dimension numbers. For eg: bvd(1, 2) refers to bvd
# over the first two features R and G.
bvd(1, 3, tr_means, tr_covs, colors)
# Implement MLC
# Note: The method must work for any number of dimensions
# Your code might be tested with an image with more than 3 features.
# Args:
# means - list containing mean vectors for each class
# covs - list containing covariance matrices for each class
# aprioris - list comprising of apriori probabilities for each class
# tedata - data frame of test data without the labels column
# Returns:
# predicted_cl - Vector of class assignments of length nrow(tedata)
# You should implement MLC manually, do not use any library
###
MLC <- function(means, covs, aprioris, tedata){
# vector to store predicted classes for each instance in test
predicted_cl = rep(0, nrow(tedata))
predicted_cl = apply(tedata, 1, MLC_calc, means=means,covs=covs,aprioris=aprioris)
#for (i in sequence(nrow(tedata))){
#}
# Return the vector of predicted labels
return(predicted_cl)
}
MLC_calc <- function(dataPoint, means, covs, aprioris){
#dataPoint = tedata[i,]
#print(dataPoint)
print(dim(t(as.matrix(dataPoint-means[[2]]))))
classProbs = rep(0, length(means))
classProbs = lapply(1:nclasses, function(j) (exp(-(as.matrix(dataPoint-means[[j]]))%*%solve(covs[[j]])%*%(t(dataPoint-means[[j]]))))*(aprioris[[j]]/sqrt(det(covs[[j]]))))
#for (j in sequence(length(means))){
#  classProbs[j] = (exp(-(as.matrix(dataPoint-means[[j]]))%*%solve(covs[[j]])%*%(t(dataPoint-means[[j]]))))*(aprioris[[j]]/sqrt(det(covs[[j]])))
#}
class = which.max(classProbs)
}
pred_te_labels <- MLC(tr_means, tr_covs, tr_apriori, te[, -ncol(te)])
# Generate confusion matrix - Variable 'tab' stores this confusion matrix
tab <- table(as.factor(te$Class), as.factor(pred_te_labels))
# Using the confusion matrix tab, compute overall accuracy
###
compute_overall_accuracy <- function(tab){
correct_classification<-0
total = 0
for(i in sequence(nrow(tab))){
correct_classification = correct_classification+tab[i,i]
total = total+sum(tab[i,])
}
return(correct_classification/total)
}
# Compute accuracy of a particular class using 'tab'
# Args:
# class - refers to the class number
# tab - confusion matrix
# Keep track of whether your actual and predicted labels are in rows or columns
# of tab
###
compute_class_accuracy <- function(class, tab){
return(tab[class]/sum(tab[class,]))
}
# Call overall class accuracy computation function
overall_te_accuracy <- compute_overall_accuracy(tab)
# Compute individual class accuracy for all classes
individual_te_accuracy <- sapply(1:nclasses, compute_class_accuracy, tab)
# Now, let's do it on the whole image
# This will take a few minutes (or longer, depending upon your implementation)
pred_img_labels <- MLC(tr_means, tr_covs, tr_apriori, img)
end.time <- Sys.time()
time.taken <- end.time - start.time
# Plot the classification result on entire image
# Check your working directory for 'mlc.tif'!
plotTif(pred_img_labels)
########
# Maximum Likelihood Classifier
# Student Name: Pranav Nawathe
# Student Unity ID: ppnawath
######
# Do not clear your workspace
# Do not set working directory in the code
require(mixtools) #for bivariate density plots/ellipses
# Run startup script to load functions to read geotiff files
source('./startup.R')
start.time <- Sys.time()
# Read data for training and testing
data <- readCSV()
# tr contains training data, first element of data
# te contains testing data, second element of data
tr <- data[[1]]
te <- data[[2]]
# Read the image(Geotiff file)
img <- readTif()
# Count the number of training instances
ntraining <- nrow(tr)
# Count the number of features
nfeatures <- ncol(tr) - 1
# Count the number of classes
nclasses <- length(unique(tr$Class))
# tr_splits is a list, where each element of the list contains
# information pertaining a single class
tr_splits <- split(tr, tr$Class)
# HINT: For the next three methods, see how split() and lapply() work
# This will give you insight into writing code for the following functions
# Function to compute the mean of each feature in a data frame
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# (nrows x (nfeatures + 1))
# Returns:
# A data frame containing means of each feature of size (1 x nfeatures)
# Don't forget to ignore class label column when computing mean
###
compute_means <- function(df){
mean = colMeans(df[-4])
}
# Function to compute the covariance matrix between features in a data frame
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# (nrows x (nfeatures + 1))
# Returns:
# Covariance matrix of size nfeatures * nfeatures
# Don't forget to ignore class label column when computing covariance matrix
###
compute_covs <- function(df){
covariance = cov(df[-4])
}
# Args:
# A data frame 'df' of training data pertaining to a single class of size
# ntraining, the number of training data points
# Returns:
# Probability of class contained in df
###
compute_apriori <- function(df, ntraining){
apriori = nrow(df)/ntraining
}
# tr_means is a list containing mean vector of attributes per class.
tr_means <- lapply(tr_splits, compute_means)
# tr_covs is a list containing covariance matrices of attributes per class.
tr_covs <- lapply(tr_splits, compute_covs)
# tr_apriori is a list containing prior probabilities per class.
tr_apriori <- lapply(tr_splits, compute_apriori, ntraining)
# function to implement bivariate density plots (ellipses)
# Plot the training data points using only feat1 and feat2
# Using the means and covs you calculated in the previous steps,
# plot ellipses for feat1 and feat2.
# You are expected to generate 5 ellipses in the same plot, one for each class
# Distinguish ellipses corresponding to classes using the colors variable
# You should also generate a legend, indicating colors assigned to each class.
# Label x and y axes and provide a descriptive plot title.
# Here's an example for ellipse() usage:
# ellipse(mu = [mean vector of size 1 x 2],
#   sigma = [covariance matrix of size 2 x 2],
#   col = colors[class])
# An example plot has been provided in the PDF.
###
bvd <- function(feat1, feat2, tr_means = tr_means, tr_covs = tr_covs, colors = colors){
print(colors)
plot(x=tr[,feat1],y=tr[,feat2],xlab=rownames(data.frame(tr_means))[feat1],ylab=rownames(data.frame(tr_means))[feat2],main=paste("Plot for dimensions:",feat1,",",feat2))
legend(x="topleft",y=NULL,seq(1,nclasses),lty=c(1,1),col=colors,inset=c(0.005,0.005))
lapply(1:nclasses, function(i) lines(ellipse(mu = tr_means[[i]][c(feat1,feat2)],sigma = tr_covs[[i]][c(feat1,feat2),c(feat1,feat2)]),col=colors[i],lwd=i))
}
# Call the bvd density plot.
# 'feat1','feat2' variables refers to the dimensions
# Input dimension numbers. For eg: bvd(1, 2) refers to bvd
# over the first two features R and G.
bvd(1, 3, tr_means, tr_covs, colors)
# Implement MLC
# Note: The method must work for any number of dimensions
# Your code might be tested with an image with more than 3 features.
# Args:
# means - list containing mean vectors for each class
# covs - list containing covariance matrices for each class
# aprioris - list comprising of apriori probabilities for each class
# tedata - data frame of test data without the labels column
# Returns:
# predicted_cl - Vector of class assignments of length nrow(tedata)
# You should implement MLC manually, do not use any library
###
MLC <- function(means, covs, aprioris, tedata){
# vector to store predicted classes for each instance in test
predicted_cl = rep(0, nrow(tedata))
predicted_cl = apply(tedata, 1, MLC_calc, means=means,covs=covs,aprioris=aprioris)
#for (i in sequence(nrow(tedata))){
#}
# Return the vector of predicted labels
return(predicted_cl)
}
MLC_calc <- function(dataPoint, means, covs, aprioris){
#dataPoint = tedata[i,]
#print(dataPoint)
#print(dim(t(as.matrix(dataPoint-means[[2]]))))
classProbs = rep(0, length(means))
classProbs = lapply(1:nclasses, function(j) (exp(-(t(as.matrix(dataPoint-means[[j]])))%*%solve(covs[[j]])%*%(t(dataPoint-means[[j]]))))*(aprioris[[j]]/sqrt(det(covs[[j]]))))
#for (j in sequence(length(means))){
#  classProbs[j] = (exp(-(as.matrix(dataPoint-means[[j]]))%*%solve(covs[[j]])%*%(t(dataPoint-means[[j]]))))*(aprioris[[j]]/sqrt(det(covs[[j]])))
#}
class = which.max(classProbs)
}
pred_te_labels <- MLC(tr_means, tr_covs, tr_apriori, te[, -ncol(te)])
# Generate confusion matrix - Variable 'tab' stores this confusion matrix
tab <- table(as.factor(te$Class), as.factor(pred_te_labels))
# Using the confusion matrix tab, compute overall accuracy
###
compute_overall_accuracy <- function(tab){
correct_classification<-0
total = 0
for(i in sequence(nrow(tab))){
correct_classification = correct_classification+tab[i,i]
total = total+sum(tab[i,])
}
return(correct_classification/total)
}
# Compute accuracy of a particular class using 'tab'
# Args:
# class - refers to the class number
# tab - confusion matrix
# Keep track of whether your actual and predicted labels are in rows or columns
# of tab
###
compute_class_accuracy <- function(class, tab){
return(tab[class]/sum(tab[class,]))
}
# Call overall class accuracy computation function
overall_te_accuracy <- compute_overall_accuracy(tab)
# Compute individual class accuracy for all classes
individual_te_accuracy <- sapply(1:nclasses, compute_class_accuracy, tab)
# Now, let's do it on the whole image
# This will take a few minutes (or longer, depending upon your implementation)
pred_img_labels <- MLC(tr_means, tr_covs, tr_apriori, img)
end.time <- Sys.time()
time.taken <- end.time - start.time
# Plot the classification result on entire image
# Check your working directory for 'mlc.tif'!
plotTif(pred_img_labels)
training_data = read.csv("../../Data/Training/ValidationDataImage3.csv")
setwd("C:/Users/prana/Google Drive/Graduation/Alda/project/Multi-temporal-Classification-of-satellite-images/Rscripts/pranav")
training_data = read.csv("../../Data/Training/ValidationDataImage3.csv")
training_data = dataset[,-c(1,2,3)]
# Random Forest
library(randomForest)
dataset = read.csv("../../Data/Training/ValidationDataImage3.csv")
training_data = dataset[,-c(1,2,3)]
model.rf = randomForest(as.factor(training_data$)~.,data=training_data[,-4], ntree=9)
model.rf = randomForest(as.factor(training_data$)~.,data=training_data[,-4], ntree=9)
model.rf = randomForest(as.factor(training_data$)~.),data=training_data[,-4], ntree=9)
model.rf = randomForest(as.factor(training_data$Class)~.,data=training_data[,-4], ntree=9)
model.rf
model.rf$importance
library(randomForest)
dataset1 = read.csv("../../Data/Training/ValidationDataImage1.csv")
training_data1 = dataset[,-c(1,2,3)]
model.rf1 = randomForest(as.factor(training_data1$Class)~.,data=training_data1[,-4], ntree=9)
model.rf1$importance
dataset2 = read.csv("../../Data/Training/ValidationDataImage2.csv")
training_data2 = dataset[,-c(1,2,3)]
model.rf2 = randomForest(as.factor(training_data1$Class)~.,data=training_data1[,-4], ntree=9)
model.rf2$importance
dataset3 = read.csv("../../Data/Training/ValidationDataImage3.csv")
training_data3 = dataset[,-c(1,2,3)]
model.rf3 = randomForest(as.factor(training_data1$Class)~.,data=training_data1[,-4], ntree=9)
model.rf3$importance
dataset4 = read.csv("../../Data/Training/ValidationDataImage4.csv")
training_data4 = dataset[,-c(1,2,3)]
model.rf4 = randomForest(as.factor(training_data1$Class)~.,data=training_data1[,-4], ntree=9)
model.rf4$importance
