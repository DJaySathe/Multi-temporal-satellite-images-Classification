# use neural network from MASS package to predict
###
NNmodel <- train(iris[trainIdx,-5], cl, method = "nnet")
NNpreds <- predict(NNmodel, iris[testIdx, -5])
# create contingency table of predictions against ground truth
###
table(NNpreds, factor(iris[testIdx, 'Species']))
# use knn from class package to predict, use 3 nearest neighbors
###
knnPreds <- knn(iris[trainIdx,-5], iris[testIdx,-5] ,cl ,k=3 , prob = TRUE)
# create contingency table of predictions against ground truth
###
table(knnPreds, factor(iris[testIdx, ncol(iris)]))
# implement myknn with manhattan distance, majority vote and
# resolving ties by priority setosa > versicolor > virginica
myknn <- function(train, test, cl, k)
{
classes <- vector()
for(i in 1:nrow(test))
{
dists <- vector()
for(j in 1:nrow(train))
{
# implement manhattan distance calculation without using the
# dist function
###
dists <- c(dists,sum(abs(test[i,] - train[j,])))
}
# implement majority vote and resolving ties by priority to assign class
# functions order, max, which.max and table could be useful
###
dists = order(dists)
dist_table = table(cl[dists[1:k]])
rows = rownames(data.frame(which(dist_table == max(dist_table))))
if(is.element('setosa',rows))
classes[i] = 'setosa'
else if(is.element('versicolor',rows))
classes[i] = 'versicolor'
else{
classes[i] = 'virginica'
}
}
return(factor(classes))
}
# predict using your implemented function
myPreds <- myknn(train, test, cl, k = 3)
# create contingency table of predictions against ground truth
###
table(myPreds, factor(iris[testIdx, ncol(iris)]))
# compare with the knn from class package
table(myPreds, knnPreds)
table(myPreds, factor(iris[testIdx, ncol(iris)]))
table(knnPreds, factor(iris[testIdx, ncol(iris)]))
table(RFPreds, factor(iris[testIdx, ncol(iris)]))
table(RFpreds, factor(iris[testIdx, ncol(iris)]))
table(myPreds, knnPreds)
table(myPreds, knnPreds)
######
# kNN
######
# Do not clear your workspace
# load required libraries
require(class) # for kNN classifier
require(caret) # for createDataPartition, train, predict
require(randomForest) # for random forest classifier
require(MASS) # for neural net classifier
# set seed to ensure reproducibility
set.seed(100)
# load in-built dataset
data(iris)
# normalize all predictors, i.e., all but last column species
iris[, -ncol(iris)] <- scale(iris[, -ncol(iris)])
# split the data into training and test sets 70/30 split
# take a partition of the indexes then use the index vectors to subset
###
trainIdx <- createDataPartition(seq(1,nrow(iris),1), p = 0.7 , list = FALSE)
# those Idxs in original data but not in trainIdx will form testIdx
###
testIdx <- setdiff(seq(1,nrow(iris),1),trainIdx)
# subset the original dataset with trainIdx and testIdx, use all but last column
train <- iris[trainIdx, -ncol(iris)]
test <- iris[testIdx, -ncol(iris)]
# create a factor for the training data class variable
###
cl <- factor(iris[trainIdx, 'Species'])
# use random forest from randomForest package to predict
###
RFmodel <- train(train, cl , method = "rf")
RFpreds <- predict(RFmodel, iris[testIdx, ncol(iris)])
# create contingency table of predictions against ground truth
###
table(RFpreds, factor(iris[testIdx, 'Species']))
# use neural network from MASS package to predict
###
NNmodel <- train(train, cl, method = "nnet")
NNpreds <- predict(NNmodel, iris[testIdx, -5])
# create contingency table of predictions against ground truth
###
table(NNpreds, factor(iris[testIdx, 'Species']))
# use knn from class package to predict, use 3 nearest neighbors
###
knnPreds <- knn(train, test ,cl ,k=3 , prob = TRUE)
# create contingency table of predictions against ground truth
###
table(knnPreds, factor(iris[testIdx, ncol(iris)]))
# implement myknn with manhattan distance, majority vote and
# resolving ties by priority setosa > versicolor > virginica
myknn <- function(train, test, cl, k)
{
classes <- vector()
for(i in 1:nrow(test))
{
dists <- vector()
for(j in 1:nrow(train))
{
# implement manhattan distance calculation without using the
# dist function
###
dists <- c(dists,sum(abs(test[i,] - train[j,])))
}
# implement majority vote and resolving ties by priority to assign class
# functions order, max, which.max and table could be useful
###
dists = order(dists)
dist_table = table(cl[dists[1:k]])
rows = rownames(data.frame(which(dist_table == max(dist_table))))
if(is.element('setosa',rows))
classes[i] = 'setosa'
else if(is.element('versicolor',rows))
classes[i] = 'versicolor'
else{
classes[i] = 'virginica'
}
}
return(factor(classes))
}
# predict using your implemented function
myPreds <- myknn(train, test, cl, k = 3)
# create contingency table of predictions against ground truth
###
table(myPreds, factor(iris[testIdx, ncol(iris)]))
# compare with the knn from class package
table(myPreds, knnPreds)
table(myPreds, knnPreds)
table(myPreds, factor(iris[testIdx, ncol(iris)]))
######
# kNN
######
# Do not clear your workspace
# load required libraries
require(class) # for kNN classifier
require(caret) # for createDataPartition, train, predict
require(randomForest) # for random forest classifier
require(MASS) # for neural net classifier
# set seed to ensure reproducibility
set.seed(100)
# load in-built dataset
data(iris)
# normalize all predictors, i.e., all but last column species
iris[, -ncol(iris)] <- scale(iris[, -ncol(iris)])
# split the data into training and test sets 70/30 split
# take a partition of the indexes then use the index vectors to subset
###
trainIdx <- createDataPartition(seq(1,nrow(iris),1), p = 0.7 , list = FALSE)
# those Idxs in original data but not in trainIdx will form testIdx
###
testIdx <- setdiff(seq(1,nrow(iris),1),trainIdx)
# subset the original dataset with trainIdx and testIdx, use all but last column
train <- iris[trainIdx, -ncol(iris)]
test <- iris[testIdx, -ncol(iris)]
# create a factor for the training data class variable
###
cl <- factor(iris[trainIdx, 'Species'])
# use random forest from randomForest package to predict
###
RFmodel <- train(train, cl , method = "rf")
RFpreds <- predict(RFmodel, iris[testIdx, ncol(iris)])
# create contingency table of predictions against ground truth
###
table(RFpreds, factor(iris[testIdx, 'Species']))
# use neural network from MASS package to predict
###
NNmodel <- train(train, cl, method = "nnet")
NNpreds <- predict(NNmodel, iris[testIdx, -5])
# create contingency table of predictions against ground truth
###
table(NNpreds, factor(iris[testIdx, 'Species']))
# use knn from class package to predict, use 3 nearest neighbors
###
knnPreds <- knn(train, test ,cl ,k=3 , prob = TRUE)
# create contingency table of predictions against ground truth
###
table(knnPreds, factor(iris[testIdx, ncol(iris)]))
# implement myknn with manhattan distance, majority vote and
# resolving ties by priority setosa > versicolor > virginica
myknn <- function(train, test, cl, k)
{
classes <- vector()
for(i in 1:nrow(test))
{
dists <- vector()
for(j in 1:nrow(train))
{
# implement manhattan distance calculation without using the
# dist function
###
dists <- c(dists,sum(abs(test[i,] - train[j,])))
}
# implement majority vote and resolving ties by priority to assign class
# functions order, max, which.max and table could be useful
###
dists = order(dists)
dist_table = table(cl[dists[1:k]])
rows = rownames(data.frame(which(dist_table == max(dist_table))))
if(is.element('setosa',rows))
classes[i] = 'setosa'
if(is.element('versicolor',rows))
classes[i] = 'versicolor'
if(is.element('virginica',rows))
classes[i] = 'virginica'
}
return(factor(classes))
}
# predict using your implemented function
myPreds <- myknn(train, test, cl, k = 3)
# create contingency table of predictions against ground truth
###
table(myPreds, factor(iris[testIdx, ncol(iris)]))
# compare with the knn from class package
table(myPreds, knnPreds)
table(myPreds, factor(iris[testIdx, ncol(iris)]))
table(RFpreds, factor(iris[testIdx, ncol(iris)]))
######
# kNN
######
# Do not clear your workspace
# load required libraries
require(class) # for kNN classifier
require(caret) # for createDataPartition, train, predict
require(randomForest) # for random forest classifier
require(MASS) # for neural net classifier
# set seed to ensure reproducibility
set.seed(100)
# load in-built dataset
data(iris)
# normalize all predictors, i.e., all but last column species
iris[, -ncol(iris)] <- scale(iris[, -ncol(iris)])
# split the data into training and test sets 70/30 split
# take a partition of the indexes then use the index vectors to subset
###
trainIdx <- createDataPartition(seq(1,nrow(iris),1), p = 0.7 , list = FALSE)
# those Idxs in original data but not in trainIdx will form testIdx
###
testIdx <- setdiff(seq(1,nrow(iris),1),trainIdx)
# subset the original dataset with trainIdx and testIdx, use all but last column
train <- iris[trainIdx, -ncol(iris)]
test <- iris[testIdx, -ncol(iris)]
# create a factor for the training data class variable
###
cl <- factor(iris[trainIdx, 'Species'])
# use random forest from randomForest package to predict
###
RFmodel <- train(train, cl , method = "rf")
RFpreds <- predict(RFmodel, iris[testIdx, ncol(iris)])
# create contingency table of predictions against ground truth
###
table(RFpreds, factor(iris[testIdx, 'Species']))
# use neural network from MASS package to predict
###
NNmodel <- train(train, cl, method = "nnet")
NNpreds <- predict(NNmodel, iris[testIdx, -5])
# create contingency table of predictions against ground truth
###
table(NNpreds, factor(iris[testIdx, 'Species']))
# use knn from class package to predict, use 3 nearest neighbors
###
knnPreds <- knn(train, test ,cl ,k=3 , prob = TRUE)
# create contingency table of predictions against ground truth
###
table(knnPreds, factor(iris[testIdx, ncol(iris)]))
# implement myknn with manhattan distance, majority vote and
# resolving ties by priority setosa > versicolor > virginica
myknn <- function(train, test, cl, k)
{
classes <- vector()
for(i in 1:nrow(test))
{
dists <- vector()
for(j in 1:nrow(train))
{
# implement manhattan distance calculation without using the
# dist function
###
dists <- c(dists,sum(abs(test[i,] - train[j,])))
}
# implement majority vote and resolving ties by priority to assign class
# functions order, max, which.max and table could be useful
###
dists = order(dists)
dist_table = table(cl[dists[1:k]])
rows = rownames(data.frame(which(dist_table == max(dist_table))))
if(length(rows) != 1){
if(is.element('setosa',rows))
classes[i] = 'setosa'
if(is.element('versicolor',rows))
classes[i] = 'versicolor'
if(is.element('virginica',rows))
classes[i] = 'virginica'
}
}
return(factor(classes))
}
# predict using your implemented function
myPreds <- myknn(train, test, cl, k = 3)
# create contingency table of predictions against ground truth
###
table(myPreds, factor(iris[testIdx, ncol(iris)]))
# compare with the knn from class package
table(myPreds, knnPreds)
######
# kNN
######
# Do not clear your workspace
# load required libraries
require(class) # for kNN classifier
require(caret) # for createDataPartition, train, predict
require(randomForest) # for random forest classifier
require(MASS) # for neural net classifier
# set seed to ensure reproducibility
set.seed(100)
# load in-built dataset
data(iris)
# normalize all predictors, i.e., all but last column species
iris[, -ncol(iris)] <- scale(iris[, -ncol(iris)])
# split the data into training and test sets 70/30 split
# take a partition of the indexes then use the index vectors to subset
###
trainIdx <- createDataPartition(seq(1,nrow(iris),1), p = 0.7 , list = FALSE)
# those Idxs in original data but not in trainIdx will form testIdx
###
testIdx <- setdiff(seq(1,nrow(iris),1),trainIdx)
# subset the original dataset with trainIdx and testIdx, use all but last column
train <- iris[trainIdx, -ncol(iris)]
test <- iris[testIdx, -ncol(iris)]
# create a factor for the training data class variable
###
cl <- factor(iris[trainIdx, 'Species'])
# use random forest from randomForest package to predict
###
RFmodel <- train(train, cl , method = "rf")
RFpreds <- predict(RFmodel, iris[testIdx, ncol(iris)])
# create contingency table of predictions against ground truth
###
table(RFpreds, factor(iris[testIdx, 'Species']))
# use neural network from MASS package to predict
###
NNmodel <- train(train, cl, method = "nnet")
NNpreds <- predict(NNmodel, iris[testIdx, -5])
# create contingency table of predictions against ground truth
###
table(NNpreds, factor(iris[testIdx, 'Species']))
# use knn from class package to predict, use 3 nearest neighbors
###
knnPreds <- knn(train, test ,cl ,k=3 , prob = TRUE)
# create contingency table of predictions against ground truth
###
table(knnPreds, factor(iris[testIdx, ncol(iris)]))
# implement myknn with manhattan distance, majority vote and
# resolving ties by priority setosa > versicolor > virginica
myknn <- function(train, test, cl, k)
{
classes <- vector()
for(i in 1:nrow(test))
{
dists <- vector()
for(j in 1:nrow(train))
{
# implement manhattan distance calculation without using the
# dist function
###
dists <- c(dists,sum(abs(test[i,] - train[j,])))
}
# implement majority vote and resolving ties by priority to assign class
# functions order, max, which.max and table could be useful
###
dists = order(dists)
dist_table = table(cl[dists[1:k]])
rows = rownames(data.frame(which(dist_table == max(dist_table))))
if(length(rows) != 1){
if(is.element('setosa',rows))
classes[i] = 'setosa'
if(is.element('versicolor',rows))
classes[i] = 'versicolor'
if(is.element('virginica',rows))
classes[i] = 'virginica'
}else{
classes[i] = 'virginica'
}
}
return(factor(classes))
}
# predict using your implemented function
myPreds <- myknn(train, test, cl, k = 3)
# create contingency table of predictions against ground truth
###
table(myPreds, factor(iris[testIdx, ncol(iris)]))
# compare with the knn from class package
table(myPreds, knnPreds)
######
# kNN
######
# Do not clear your workspace
# load required libraries
require(class) # for kNN classifier
require(caret) # for createDataPartition, train, predict
require(randomForest) # for random forest classifier
require(MASS) # for neural net classifier
# set seed to ensure reproducibility
set.seed(100)
# load in-built dataset
data(iris)
# normalize all predictors, i.e., all but last column species
iris[, -ncol(iris)] <- scale(iris[, -ncol(iris)])
# split the data into training and test sets 70/30 split
# take a partition of the indexes then use the index vectors to subset
###
trainIdx <- createDataPartition(seq(1,nrow(iris),1), p = 0.7 , list = FALSE)
# those Idxs in original data but not in trainIdx will form testIdx
###
testIdx <- setdiff(seq(1,nrow(iris),1),trainIdx)
# subset the original dataset with trainIdx and testIdx, use all but last column
train <- iris[trainIdx, -ncol(iris)]
test <- iris[testIdx, -ncol(iris)]
# create a factor for the training data class variable
###
cl <- factor(iris[trainIdx, 'Species'])
# use random forest from randomForest package to predict
###
RFmodel <- train(train, cl , method = "rf")
RFpreds <- predict(RFmodel, iris[testIdx, ncol(iris)])
# create contingency table of predictions against ground truth
###
table(RFpreds, factor(iris[testIdx, 'Species']))
# use neural network from MASS package to predict
###
NNmodel <- train(train, cl, method = "nnet")
NNpreds <- predict(NNmodel, iris[testIdx, -5])
# create contingency table of predictions against ground truth
###
table(NNpreds, factor(iris[testIdx, 'Species']))
# use knn from class package to predict, use 3 nearest neighbors
###
knnPreds <- knn(train, test ,cl ,k=3 , prob = TRUE)
# create contingency table of predictions against ground truth
###
table(knnPreds, factor(iris[testIdx, ncol(iris)]))
# implement myknn with manhattan distance, majority vote and
# resolving ties by priority setosa > versicolor > virginica
myknn <- function(train, test, cl, k)
{
classes <- vector()
for(i in 1:nrow(test))
{
dists <- vector()
for(j in 1:nrow(train))
{
# implement manhattan distance calculation without using the
# dist function
###
dists <- c(dists,sum(abs(test[i,] - train[j,])))
}
# implement majority vote and resolving ties by priority to assign class
# functions order, max, which.max and table could be useful
###
dists = order(dists)
dist_table = table(cl[dists[1:k]])
rows = rownames(data.frame(which(dist_table == max(dist_table))))
if(is.element('setosa',rows))
classes[i] = 'setosa'
if(is.element('versicolor',rows))
classes[i] = 'versicolor'
if(is.element('virginica',rows))
classes[i] = 'virginica'
}
return(factor(classes))
}
# predict using your implemented function
myPreds <- myknn(train, test, cl, k = 3)
# create contingency table of predictions against ground truth
###
table(myPreds, factor(iris[testIdx, ncol(iris)]))
# compare with the knn from class package
table(myPreds, knnPreds)
table(myPreds, factor(iris[testIdx, ncol(iris)]))
table(knnPreds, factor(iris[testIdx, ncol(iris)]))
table(RFPreds, factor(iris[testIdx, ncol(iris)]))
table(RFpreds, factor(iris[testIdx, ncol(iris)]))
setwd("C:/Users/prana/Google Drive/Graduation/Alda/project/Multi-temporal-Classification-of-satellite-images")
ls
pwd
setwd("C:/Users/prana/Google Drive/Graduation/Alda/project/Multi-temporal-Classification-of-satellite-images/Data")
read.csv('Training/ValidationDataImage1.csv')
dt = read.csv('Training/ValidationDataImage1.csv')
dt = dt[-c(1,2,3,4)]
cor(dt)
cor(dt) > 0.7
cor(dt) > 0.9
dt = read.csv('Training/ValidationDataImage2.csv')
dt = dt[-c(1,2,3,4)]
cor(dt) > 0.7
cor(dt)
cor(dt) > 0.8
